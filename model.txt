import pandas as pd
import numpy as np

# For train-test split and evaluation
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# LightGBM
import lightgbm as lgb

# For plotting
import matplotlib.pyplot as plt
import seaborn as sns

# For SHAP
import shap



# Target
target_col = "target_total_premium_pmpm"

# We drop the target from feature set
X = df.drop(columns=[target_col])
y = df[target_col]

for cat_col in categorical_cols:
    X[cat_col] = X[cat_col].astype("category")

X_train, X_test, y_train, y_test = train_test_split(
    X, 
    y, 
    test_size=0.2, 
    random_state=42
)

X_train_fold, X_valid_fold, y_train_fold, y_valid_fold = train_test_split(
    X_train,
    y_train,
    test_size=0.2,   # 20% of the train set is used for validation
    random_state=42
)

model = lgb.LGBMRegressor(
    n_estimators=1000,
    learning_rate=0.05,
    random_state=42
)

evals_result = {}
model.fit(
    X_train_fold, 
    y_train_fold,
    eval_set=[(X_train_fold, y_train_fold), (X_valid_fold, y_valid_fold)],
    eval_names=["train", "valid"],
    eval_metric="rmse",  # you can change to 'mae' if preferred
    early_stopping_rounds=50,
    categorical_feature=categorical_cols,
    callbacks=[
        lgb.record_evals(evals_result)  # records metrics each iteration
    ]
)

# Extract train/valid RMSE for each iteration
train_rmse = evals_result["train"]["rmse"]
valid_rmse = evals_result["valid"]["rmse"]

plt.figure(figsize=(8, 5))
plt.plot(train_rmse, label="Train RMSE")
plt.plot(valid_rmse, label="Valid RMSE")
plt.xlabel("Iteration")
plt.ylabel("RMSE")
plt.title("LightGBM RMSE over Iterations")
plt.legend()
plt.show()

y_pred_test = model.predict(X_test, num_iteration=model.best_iteration_)

rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)  # RMSE
r2_test = r2_score(y_test, y_pred_test)

print(f"Test RMSE: {rmse_test:.2f}")
print(f"Test R^2 Score: {r2_test:.3f}")

importances = model.feature_importances_
feature_names = X_train_fold.columns

# Combine into a DataFrame
feat_imp_df = pd.DataFrame({
    'feature': feature_names, 
    'importance': importances
}).sort_values('importance', ascending=False)

plt.figure(figsize=(8, 8))
sns.barplot(data=feat_imp_df.head(20), x='importance', y='feature')
plt.title("Top 20 Feature Importances (LightGBM)")
plt.show()

# Create a small sample if your dataset is huge (to speed up SHAP)
sample_size = 5000
X_shap = X_test.sample(n=sample_size, random_state=42) if len(X_test) > sample_size else X_test

explainer = shap.TreeExplainer(model, feature_perturbation="tree_path_dependent")
shap_values = explainer.shap_values(X_shap)

shap.summary_plot(shap_values, X_shap, plot_type="bar", max_display=20)

shap.summary_plot(shap_values, X_shap, plot_type="dot", max_display=20)

# For example, top feature might be 'plan_fp_moop'
top_feature = feat_imp_df['feature'].iloc[0]
shap.dependence_plot(top_feature, shap_values, X_shap)
